{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8be0b98-02de-48ca-853d-a74404f35d1d",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2d2236-1777-4e6b-8a69-8ed28dc6808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values in a dataset refer to the absence of a value or a piece of information that should have been present in the dataset. \n",
    "#There are various reasons why data may be missing, such as human error, technical problems during data collection, or intentionally left \n",
    "#blank by the respondents.\n",
    "\n",
    "#It is essential to handle missing values in a dataset because they can affect the accuracy and reliability of data analysis and modeling. \n",
    "#If missing values are ignored, it can lead to biased results, incorrect statistical inferences, and inaccurate predictions. \n",
    "#Handling missing values is crucial to ensure that the data analysis and modeling accurately reflect the real-world phenomena being studied.\n",
    "\n",
    "#Some of the algorithms that are not affected by missing values are:\n",
    "\n",
    "#Decision trees: Decision trees can handle missing values without requiring imputation, as the algorithm can choose a split that sends missing \n",
    "#values to a separate branch.\n",
    "\n",
    "#Random forests: Random forests can also handle missing values because they use multiple decision trees, and the missing values are dealt with\n",
    "#in the same way as decision trees.\n",
    "\n",
    "#K-nearest neighbors: KNN is a distance-based algorithm that can handle missing values by ignoring the missing values when calculating the \n",
    "#distances between data points.\n",
    "\n",
    "#Support vector machines: SVMs can handle missing values by ignoring the missing values when calculating the hyperplane that separates the data.\n",
    "\n",
    "#Naive Bayes: Naive Bayes is a probabilistic algorithm that can handle missing values by ignoring the missing values in the calculation of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8e53f-b06b-4f4d-a50d-67df59a72925",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65945606-966f-4904-8a01-5612270686b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  NaN\n",
      "1  2.0  5.0\n",
      "2  NaN  6.0\n",
      "3  4.0  7.0\n",
      "4  NaN  8.0\n",
      "     A    B\n",
      "1  2.0  5.0\n",
      "3  4.0  7.0\n"
     ]
    }
   ],
   "source": [
    "#There are several techniques used to handle missing data in a dataset. Here are some of the most common techniques along with an example in Python:\n",
    "\n",
    "#Deletion: In this technique, missing values are simply removed from the dataset. However, this technique can lead to loss of valuable information,\n",
    "#especially if the number of missing values is high.\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, None], 'B': [None, 5, 6, 7, 8]})\n",
    "print(df)\n",
    "\n",
    "# remove rows with missing values\n",
    "df_dropna = df.dropna()\n",
    "print(df_dropna)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c41b0c4-3520-4d42-8667-ef6cdd3db772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  NaN\n",
      "1  2.0  5.0\n",
      "2  NaN  6.0\n",
      "3  4.0  7.0\n",
      "4  NaN  8.0\n",
      "          A    B\n",
      "0  1.000000  6.5\n",
      "1  2.000000  5.0\n",
      "2  2.333333  6.0\n",
      "3  4.000000  7.0\n",
      "4  2.333333  8.0\n"
     ]
    }
   ],
   "source": [
    "#Mean/Mode/Median Imputation: In this technique, missing values are replaced with the mean, mode or median of the respective feature. \n",
    "#This technique assumes that the missing values are missing at random (MAR).\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, None], 'B': [None, 5, 6, 7, 8]})\n",
    "print(df)\n",
    "\n",
    "# replace missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_mean = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638fd4e5-492d-4352-8fb9-920122e6a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "0  1.0  NaN  3.0\n",
      "1  2.0  5.0  4.0\n",
      "2  NaN  6.0  5.0\n",
      "3  4.0  7.0  NaN\n",
      "4  NaN  8.0  7.0\n",
      "          A         B         C\n",
      "0  1.000000  3.334401  3.000000\n",
      "1  2.000000  5.000000  4.000000\n",
      "2  1.508450  6.000000  5.000000\n",
      "3  4.000000  7.000000  4.908275\n",
      "4  0.525349  8.000000  7.000000\n"
     ]
    }
   ],
   "source": [
    "#Regression Imputation: In this technique, a regression model is used to predict missing values based on other features.\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, None], 'B': [None, 5, 6, 7, 8], 'C': [3, 4, 5, None, 7]})\n",
    "print(df)\n",
    "\n",
    "# replace missing values with regression imputation\n",
    "imputer = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state=0)\n",
    "df_regression = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562d819-c456-48ea-9ca6-2b05560505ee",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0a9333-af50-48d9-bbc9-335fb36cc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imbalanced data refers to a situation in a dataset where the number of instances or observations in one class is much higher or much lower \n",
    "#than the number of instances in another class. This is a common issue in machine learning and data mining applications, and it can occur in \n",
    "#many real-world problems, such as fraud detection, medical diagnosis, or customer churn prediction.\n",
    "\n",
    "#If imbalanced data is not handled, it can lead to biased and inaccurate results. \n",
    "#The model trained on imbalanced data will tend to favor the majority class and perform poorly on the minority class. In many cases, \n",
    "#the minority class is the one that is of most interest, such as fraud cases or rare diseases. The model may fail to detect the minority class, \n",
    "#leading to missed opportunities or wrong decisions.\n",
    "\n",
    "#For example, suppose we have a dataset with 1,000 observations, out of which 900 belong to class A and 100 belong to class B. \n",
    "#This is an imbalanced dataset because class A has many more observations than class B. If we train a model on this dataset without handling \n",
    "#the imbalance, the model may predict all instances as class A, leading to 100% accuracy on class A but 0% accuracy on class B. \n",
    "#This is not a useful model for predicting the minority class, and it can lead to costly mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3079f6-7191-42f2-83c5-aa92c8f9bec1",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b996ff0e-e51a-46c2-b2d1-0a446e63e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Up-sampling and down-sampling are two common techniques used to handle imbalanced data by modifying the class distribution in a dataset.\n",
    "\n",
    "#Up-sampling involves increasing the number of instances in the minority class by creating synthetic examples or replicating existing examples. \n",
    "#The goal of up-sampling is to increase the representation of the minority class, making it more comparable to the majority class.\n",
    "\n",
    "#Down-sampling, on the other hand, involves decreasing the number of instances in the majority class by randomly removing examples. \n",
    "#The goal of down-sampling is to reduce the representation of the majority class, making it more comparable to the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e4870-5d22-43ea-aee4-5865a10c316c",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae4c01f-b102-4427-a20a-d88f640ff761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation is a technique used in machine learning and computer vision to increase the size of a training dataset by creating new examples \n",
    "#from existing ones. Data augmentation is useful when the training dataset is small, and the model is prone to overfitting or when the dataset is \n",
    "#imbalanced, and some classes have few examples.\n",
    "\n",
    "#There are many data augmentation techniques available, such as flipping, rotating, scaling, cropping, and adding noise, among others. \n",
    "#These techniques can be applied to images, audio, text, or any other type of data.\n",
    "\n",
    "#One popular data augmentation technique for imbalanced datasets is SMOTE (Synthetic Minority Over-sampling Technique). \n",
    "#SMOTE is a technique that generates new examples of the minority class by interpolating between existing examples of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6977fc-95a9-4d86-91df-58238656bb5d",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3d347f-28f7-4571-9cf6-cce4455ae8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers are data points in a dataset that are significantly different from other observations. \n",
    "#They can be caused by measurement errors, data entry errors, or they may represent genuine extreme values. \n",
    "#Outliers can affect the accuracy and reliability of statistical models and machine learning algorithms, \n",
    "#leading to biased results and incorrect predictions.\n",
    "\n",
    "#It is essential to handle outliers because they can distort the results of data analysis and modeling. \n",
    "#Outliers can influence the mean and standard deviation of the data, making it difficult to determine the true distribution of the data.\n",
    "#Outliers can also lead to incorrect conclusions, especially in hypothesis testing or statistical inference. \n",
    "#For example, if we are testing the difference between two groups, outliers can affect the results of the test and lead to incorrect conclusions.\n",
    "\n",
    "#In machine learning, outliers can affect the performance of algorithms by introducing noise and bias into the model. \n",
    "#Outliers can cause overfitting, where the model becomes too complex and fits the noise in the data rather than the underlying pattern. \n",
    "#Outliers can also cause underfitting, where the model is too simple and fails to capture the important features of the data.\n",
    "\n",
    "#To handle outliers, we can use techniques such as:\n",
    "\n",
    "#Z-score method: Z-score is a measure of how many standard deviations an observation is away from the mean. \n",
    "#We can identify outliers by calculating the Z-score for each observation and removing any observation with a\n",
    "#Z-score greater than a certain threshold.\n",
    "\n",
    "#Interquartile range (IQR) method: IQR is the difference between the third quartile (75th percentile) and the first quartile (25th percentile) \n",
    "#of the data. We can identify outliers by calculating the IQR for each feature and removing any observation with a value outside the range of \n",
    "#1.5 times the IQR below the first quartile or above the third quartile.\n",
    "\n",
    "#Visualization techniques: We can use visualization techniques such as box plots or scatter plots to identify outliers visually. \n",
    "#Box plots can show the distribution of the data and identify any observations outside the whiskers, which represent the range of typical values.\n",
    "#Scatter plots can show the relationship between two variables and identify any observations that are far away from the other observations.\n",
    "\n",
    "#Handling outliers can improve the accuracy and reliability of statistical models and machine learning algorithms, \n",
    "#leading to better decision-making and more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ec96e-28a6-4823-b63f-3f9c506d705c",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ecc44f-6f26-4725-a1af-59a2463776c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "#Deletion: In this technique, we simply remove any rows or columns with missing data. This can be done using the dropna() function in pandas. \n",
    "#However, this technique can lead to loss of valuable data and may bias the analysis.\n",
    "\n",
    "#Imputation: In this technique, we fill in missing data with estimated values. This can be done using various methods such as mean, median, \n",
    "#mode, regression, and k-Nearest Neighbors (k-NN) imputation. For example, we can use the SimpleImputer() class from sklearn library in Python \n",
    "#to fill missing values with the mean or median of the available data.\n",
    "\n",
    "#Prediction modeling: In this technique, we use machine learning algorithms to predict the missing values based on the available data. \n",
    "#For example, we can use decision trees, random forests, or neural networks to predict the missing values.\n",
    "\n",
    "#Interpolation: In this technique, we estimate the missing values by interpolating between the available data points. For example, \n",
    "#we can use linear interpolation or spline interpolation to estimate missing values.\n",
    "\n",
    "#Domain knowledge: In some cases, we can use domain knowledge or expert opinion to estimate missing values. \n",
    "#For example, if we know that the missing values are related to a particular customer segment, we can estimate the values based on \n",
    "#the behavior of other customers in that segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbf6dc-d49d-4b9f-b678-28cf58a3d690",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3927061-2dfd-4b17-8609-40a1296c63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "#Descriptive statistics: We can calculate summary statistics for the dataset, such as mean, median, mode, and standard deviation, \n",
    "#for both the complete and incomplete cases. If the statistics are similar for the complete and incomplete cases, it suggests that the \n",
    "#missing data is missing at random.\n",
    "\n",
    "#Visualization: We can use visualization techniques such as scatter plots, histograms, and box plots to compare the distribution of \n",
    "#the complete and incomplete cases. If there is a difference in the distribution, it suggests that the missing data is not missing at random.\n",
    "\n",
    "#Correlation analysis: We can calculate the correlation between the missing values and other variables in the dataset. \n",
    "#If there is a strong correlation, it suggests that the missing data is not missing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a9a4e-ebd3-4ea2-a247-f1ccbc961b09",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "697d3127-490b-4dc9-8244-ae3f752e6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with imbalanced datasets is a common challenge in machine learning. Here are some strategies that can be used to evaluate the performance\n",
    "#of machine learning models on imbalanced datasets:\n",
    "\n",
    "#Confusion matrix: A confusion matrix provides a summary of the performance of a classification model. We can use this to evaluate the true \n",
    "#positive rate, true negative rate, false positive rate, and false negative rate. This helps to evaluate the performance of the model in \n",
    "#identifying the minority class.\n",
    "\n",
    "#ROC curve: The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a classification model.\n",
    "#It plots the true positive rate (TPR) against the false positive rate (FPR) at various thresholds. This helps to evaluate the model's \n",
    "#performance in identifying the minority class.\n",
    "\n",
    "#Precision-Recall curve: The precision-recall curve is another graphical representation of the performance of a classification model. \n",
    "#It plots precision against recall at various thresholds. This helps to evaluate the model's performance in identifying the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0729bd-b9ea-4d6b-8298-90d0c02195aa",
   "metadata": {},
   "source": [
    "Q11: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00580b72-a574-4a05-92ee-f7cae8204792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To balance an unbalanced dataset with the bulk of customers reporting being satisfied, we can use the following methods to down-sample \n",
    "#the majority class:\n",
    "\n",
    "#Random under-sampling: In this technique, we randomly select a subset of samples from the majority class to balance the dataset. \n",
    "#This method can result in the loss of valuable information.\n",
    "\n",
    "#Cluster-based under-sampling: In this technique, we cluster the majority class samples and select a representative subset of samples\n",
    "#from each cluster to balance the dataset.\n",
    "\n",
    "#Tomek links: In this technique, we identify pairs of samples that are closest to each other but belong to different classes. \n",
    "#We then remove the majority class samples from these pairs to balance the dataset.\n",
    "\n",
    "#Edited nearest neighbors: In this technique, we remove the majority class samples that are misclassified by their nearest neighbors to balance\n",
    "#the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0d008-8f6d-4c3e-80ca-b53769e6d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
